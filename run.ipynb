{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125\n",
      "3125\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "dataset='generated_text'\n",
    "embed=\"bert-base\"\n",
    "mode=\"512_head\"\n",
    "embed_mode=\"cls\"\n",
    "inp_dir=\"pkl_data/\"\n",
    "author_ids = []\n",
    "for chunk_id in range(2):\n",
    "    file = open(\n",
    "        inp_dir + dataset + \"-\" + embed + \"-\" + embed_mode + \"-\" + mode + \"-\" + str(chunk_id) +\".pkl\", \"rb\"\n",
    "    )\n",
    "    data = pickle.load(file)\n",
    "    author_ids_x, data_x = list(zip(*data))\n",
    "    print(len(author_ids_x))\n",
    "    file.close()\n",
    "    author_ids+=list(author_ids_x)\n",
    "author_ids = [a for a in author_ids for aa in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base bert-base True\n",
      "*********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\Cristian/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\Cristian/.cache\\huggingface\\transformers\\a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at C:\\Users\\Cristian/.cache\\huggingface\\transformers\\45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at C:\\Users\\Cristian/.cache\\huggingface\\transformers\\c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\Cristian/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils.data_utils import MyNewMapDataset\n",
    "from inference.LM_extractor import get_model\n",
    "import torch\n",
    "\n",
    "embed=\"bert-base\"\n",
    "model, tokenizer, n_hl, hidden_dim = get_model(embed)\n",
    "dataset=\"generated_text\"\n",
    "datafile=r\"C:\\Users\\Cristian\\Documents\\HolisticAI\\repos\\neural_nets_personality\\outputs\\organized_text\\trait_activating_questions_clean.csv\"\n",
    "token_length=512\n",
    "DEVICE = torch.device(\"cuda\")\n",
    "mode=\"512_head\"\n",
    "chunk_id=0\n",
    "total_chunks=1\n",
    "map_dataset = MyNewMapDataset(dataset, datafile, tokenizer, token_length, DEVICE, mode, chunk_id, total_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>EXT_0</th>\n",
       "      <th>EXT_1</th>\n",
       "      <th>NEU_0</th>\n",
       "      <th>NEU_1</th>\n",
       "      <th>AGR_0</th>\n",
       "      <th>AGR_1</th>\n",
       "      <th>CON_0</th>\n",
       "      <th>CON_1</th>\n",
       "      <th>OPN_0</th>\n",
       "      <th>...</th>\n",
       "      <th>network</th>\n",
       "      <th>lr</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>epochs</th>\n",
       "      <th>model_input</th>\n",
       "      <th>embed</th>\n",
       "      <th>layer</th>\n",
       "      <th>mode</th>\n",
       "      <th>embed_mode</th>\n",
       "      <th>jobid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.511496</td>\n",
       "      <td>0.378937</td>\n",
       "      <td>0.30485</td>\n",
       "      <td>0.422386</td>\n",
       "      <td>-0.695632</td>\n",
       "      <td>0.352473</td>\n",
       "      <td>0.254014</td>\n",
       "      <td>0.685831</td>\n",
       "      <td>-1.133717</td>\n",
       "      <td>...</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>LM_features</td>\n",
       "      <td>bert-base</td>\n",
       "      <td>11</td>\n",
       "      <td>512_head</td>\n",
       "      <td>cls</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.511496</td>\n",
       "      <td>0.378937</td>\n",
       "      <td>0.30485</td>\n",
       "      <td>0.422386</td>\n",
       "      <td>-0.695632</td>\n",
       "      <td>0.352473</td>\n",
       "      <td>0.254014</td>\n",
       "      <td>0.685831</td>\n",
       "      <td>-1.133717</td>\n",
       "      <td>...</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>LM_features</td>\n",
       "      <td>bert-base</td>\n",
       "      <td>11</td>\n",
       "      <td>512_head</td>\n",
       "      <td>cls</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.511496</td>\n",
       "      <td>0.378937</td>\n",
       "      <td>0.30485</td>\n",
       "      <td>0.422386</td>\n",
       "      <td>-0.695632</td>\n",
       "      <td>0.352473</td>\n",
       "      <td>0.254014</td>\n",
       "      <td>0.685831</td>\n",
       "      <td>-1.133717</td>\n",
       "      <td>...</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>LM_features</td>\n",
       "      <td>bert-base</td>\n",
       "      <td>11</td>\n",
       "      <td>512_head</td>\n",
       "      <td>cls</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.511496</td>\n",
       "      <td>0.378937</td>\n",
       "      <td>0.30485</td>\n",
       "      <td>0.422386</td>\n",
       "      <td>-0.695632</td>\n",
       "      <td>0.352473</td>\n",
       "      <td>0.254014</td>\n",
       "      <td>0.685831</td>\n",
       "      <td>-1.133717</td>\n",
       "      <td>...</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>LM_features</td>\n",
       "      <td>bert-base</td>\n",
       "      <td>11</td>\n",
       "      <td>512_head</td>\n",
       "      <td>cls</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.511496</td>\n",
       "      <td>0.378937</td>\n",
       "      <td>0.30485</td>\n",
       "      <td>0.422386</td>\n",
       "      <td>-0.695632</td>\n",
       "      <td>0.352473</td>\n",
       "      <td>0.254014</td>\n",
       "      <td>0.685831</td>\n",
       "      <td>-1.133717</td>\n",
       "      <td>...</td>\n",
       "      <td>MLP</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>LM_features</td>\n",
       "      <td>bert-base</td>\n",
       "      <td>11</td>\n",
       "      <td>512_head</td>\n",
       "      <td>cls</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     EXT_0     EXT_1    NEU_0     NEU_1     AGR_0     AGR_1  \\\n",
       "0           0 -0.511496  0.378937  0.30485  0.422386 -0.695632  0.352473   \n",
       "1           1 -0.511496  0.378937  0.30485  0.422386 -0.695632  0.352473   \n",
       "2           2 -0.511496  0.378937  0.30485  0.422386 -0.695632  0.352473   \n",
       "3           3 -0.511496  0.378937  0.30485  0.422386 -0.695632  0.352473   \n",
       "4           4 -0.511496  0.378937  0.30485  0.422386 -0.695632  0.352473   \n",
       "\n",
       "      CON_0     CON_1     OPN_0  ...  network      lr  batch_size  epochs  \\\n",
       "0  0.254014  0.685831 -1.133717  ...      MLP  0.0005          32      10   \n",
       "1  0.254014  0.685831 -1.133717  ...      MLP  0.0005          32      10   \n",
       "2  0.254014  0.685831 -1.133717  ...      MLP  0.0005          32      10   \n",
       "3  0.254014  0.685831 -1.133717  ...      MLP  0.0005          32      10   \n",
       "4  0.254014  0.685831 -1.133717  ...      MLP  0.0005          32      10   \n",
       "\n",
       "   model_input      embed layer      mode embed_mode jobid  \n",
       "0  LM_features  bert-base    11  512_head        cls     0  \n",
       "1  LM_features  bert-base    11  512_head        cls     0  \n",
       "2  LM_features  bert-base    11  512_head        cls     0  \n",
       "3  LM_features  bert-base    11  512_head        cls     0  \n",
       "4  LM_features  bert-base    11  512_head        cls     0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results_essays_path = r'C:\\Users\\Cristian\\Documents\\HolisticAI\\repos\\personality-prediction\\explogs\\dataset_inference_essays_generated_text.csv'\n",
    "df_essays =  pd.read_csv(results_essays_path)\n",
    "df_essays.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for c in [\"EXT\", \"NEU\", \"AGR\", \"CON\", \"OPN\"]:\n",
    "    print(df_essays[[f\"{c}_0\",f\"{c}_1\"]].apply(lambda row: np.argmax(row.values), axis=1).unique())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_essays)#['EXT_0'].max()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "72d77abdf3cbaa7a26a7e55aa6c7dc26f430280e8412a9483179486c90d47c30"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 ('personality')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
